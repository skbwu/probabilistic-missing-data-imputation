{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73222f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, copy, os, shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import copy\n",
    "\n",
    "# our helper functions for the gridworlds\n",
    "import GridWorldHelpers as gwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e7a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our environments + corresponding colors\n",
    "d = 8 # dimension of our \n",
    "gw0, gw1, gw2 = gwh.build_grids(d=8, baseline_penalty = -1, water_penalty = -10, end_reward = 10)\n",
    "gw0_colors = gwh.make_gw_colors(gw0)\n",
    "gw1_colors = gwh.make_gw_colors(gw1)\n",
    "gw2_colors = gwh.make_gw_colors(gw2)\n",
    "\n",
    "# store quick-access indices for the environment\n",
    "environments = {\n",
    "                0: [gw0, gw0_colors], # baseline\n",
    "                1: [gw1, gw2_colors], # non-flooding\n",
    "                2: [gw2, gw2_colors] # flooding\n",
    "               }\n",
    "\n",
    "# global environment parameters \n",
    "p_switch = 0.5 # flooding Markov chain parameter\n",
    "p_wind_i = 0.5 # up-down wind frequency\n",
    "p_wind_j = 0.5 # left-right wind frequency\n",
    "\n",
    "# what is our starting \"current environment\"\n",
    "ce = 1\n",
    "\n",
    "# which environments are we flipping through?\n",
    "indices = np.array([1, 2]) # the two to be flipping between, if any. If just one, make first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4d3a9923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3944ba5b15b449c49208fa380e9b780a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make sure we are refreshing our helper functions\n",
    "reload(gwh)\n",
    "\n",
    "# SIMULATION SETTINGS\n",
    "max_iters = 1000\n",
    "baseline_imputer_settings = [\"last_fobs\", \"random_action\", \"missing_state\"]\n",
    "baseline_imputer = baseline_imputer_settings[0]\n",
    "epsilon = 0.05\n",
    "\n",
    "# initialize our Q matrix: {((i, j, color), (a1, a2))}\n",
    "Q = gwh.init_Q(d, init_value=0.0)\n",
    "action_descs = gwh.load_actions()\n",
    "actions = list(action_descs.keys())\n",
    "\n",
    "# counter of how many episodes we've hit\n",
    "finished_episodes = 0\n",
    "\n",
    "# initialize our starting environment + corresponding colors\n",
    "env, env_colors = environments[ce][0], environments[ce][1]\n",
    "\n",
    "# initialize our true initial state to be the bottom left corner. Assume fully-observed initial state\n",
    "true_state, pobs_state = (d-1, 0, env_colors[d-1, 0]), (d-1, 0, env_colors[d-1, 0])\n",
    "\n",
    "# things we want to store: rewards, time per iteration, last fully-observed state, Qmatrices every X steps.\n",
    "rewards, times, last_fobs_state = [], [], copy.deepcopy(true_state)\n",
    "\n",
    "# for each timestep ...\n",
    "for t_step in tqdm(range(max_iters)):\n",
    "    \n",
    "    ##### \"choose action A from S using policy-derived from Q (e.g., \\epsilon-greedy)\"\n",
    "    \n",
    "    # do we have any missing state values?\n",
    "    if np.any(np.isnan(pobs_state).mean()):\n",
    "        \n",
    "        # deal with it accordingly to get imputed states + actions\n",
    "        if baseline_imputer == \"last_fobs\":\n",
    "            action = gwh.select_action(last_fobs_state, Q, epsilon)\n",
    "        elif baseline_imputer == \"random_action\":\n",
    "            action = actions[np.random.choice(a=len(actions))]\n",
    "        elif baseline_imputer == \"missing_state\":\n",
    "            action = gwh.select_action(\"missing\", Q, epsilon)\n",
    "        else:\n",
    "            raise Exception(\"baseline_imputer choice is not currently supported.\")\n",
    "        \n",
    "    # if not, update our last_fobs_state + select an action accordingly\n",
    "    else:\n",
    "        \n",
    "        # select our action using standard epsilon-greedy on Q\n",
    "        action = gwh.select_action(pobs_state, Q, epsilon)\n",
    "        \n",
    "        # update our last fully-observed state\n",
    "        last_fobs_state = copy.deepcopy(pobs_state)\n",
    "    \n",
    "    ##### \"Take action A, observe R, S'\" - BASED ON TRUE STATE, OF COURSE!\n",
    "    \n",
    "    # toggle our environment potentially!\n",
    "    \n",
    "    # figure out what our new state is, which tells us our reward\n",
    "    new_true_state = gwh.true_move(true_state, action, env, env_colors, p_wind_i, p_wind_j)\n",
    "    reward = env[int(new_true_state[0]), int(new_true_state[1])]\n",
    "    \n",
    "    # simulate our partially-observed mechanism.\n",
    "    # WIP!\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6b17a1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env[int(new_true_state[0]), int(new_true_state[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "05e7d75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_true_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e169d42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0., 0.])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_true_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Afterburner)\n",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
