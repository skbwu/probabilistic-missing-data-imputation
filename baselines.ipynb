{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4caf24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, copy, os, shutil\n",
    "from importlib import reload\n",
    "import copy, time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# our helper functions for the gridworlds\n",
    "import GridWorldHelpers as gwh\n",
    "\n",
    "# do we want intermediate outputs or nah?\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc87fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our environments + corresponding colors\n",
    "d = 8 # dimension of our gridworld\n",
    "gw0, gw1, gw2 = gwh.build_grids(d=8, baseline_penalty = -1, \n",
    "                                water_penalty = -10, \n",
    "                                end_reward = 100)\n",
    "gw0_colors = gwh.make_gw_colors(gw0)\n",
    "gw1_colors = gwh.make_gw_colors(gw1)\n",
    "gw2_colors = gwh.make_gw_colors(gw2)\n",
    "\n",
    "# store quick-access indices for the environment\n",
    "environments = {\n",
    "                0: [gw0, gw0_colors], # baseline\n",
    "                1: [gw1, gw2_colors], # non-flooding\n",
    "                2: [gw2, gw2_colors] # flooding\n",
    "               }\n",
    "\n",
    "# global environment parameters \n",
    "p_switch = 0.1 # flooding Markov chain parameter, {0.0, 0.1}\n",
    "p_wind_i = 0.0 # up-down wind frequency, {0, 0.1, 0.2} (COUPLED WITH p_wind_j)\n",
    "p_wind_j = 0.0 # left-right wind frequency, {0, 0.1, 0.2}\n",
    "\n",
    "# global missing-data parameters\n",
    "thetas = np.array([0.5, 0.5, 0.5]) # common thetas -- {0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}\n",
    "thetas_in = np.array([0.50, 0.50, 0.50]) # (0.5, 0.5, 0.5) + (0.25, 0.25, 0.25)\n",
    "thetas_out = np.array([0.1, 0.1, 0.1]) # (0.0, 0.0, 0.0) + (0.1, 0.1, 0.1)\n",
    "\n",
    "# for each color\n",
    "theta_dict = {0 : np.array([0.1, 0.1, 0.1]), # 0.1 + option of (0.1, 0.1, 0.0)\n",
    "              1 : np.array([0.2, 0.2, 0.2]), # 0.2 + option of (0.2, 0.2, 0.0)\n",
    "              2 : np.array([0.3, 0.3, 0.3])} # 0.3 + option of (0.3, 0.3, 0.0)\n",
    "\n",
    "# fog range - fixed.\n",
    "i_range, j_range = (0, 2), (5, 7)\n",
    "\n",
    "# what is our starting \"current environment\"\n",
    "ce = 1\n",
    "\n",
    "# which environments are we flipping through?\n",
    "indices = np.array([1, 2]) # the two to be flipping between, if any. If just one, make first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e732adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 20000, Past 20 Mean Epi. Sum Reward: -72.872, Fin. Episodes: 141, Past 20 Mean Path Length: 141.255\n"
     ]
    }
   ],
   "source": [
    "# set a seed - note that we will have multiple trials\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "# make sure we are refreshing our helper functions\n",
    "reload(gwh)\n",
    "\n",
    "##### SIMULATION SETTINGS\n",
    "max_iters = 20000\n",
    "\n",
    "# which baseline imputing setting are we using? (note in random_action, do NOT update Q-matrix!)\n",
    "impute_method_settings = [\"last_fobs\", \"random_action\", \"missing_state\"]\n",
    "impute_method = impute_method_settings[2]\n",
    "\n",
    "# which missing-data setup are we using for environment?\n",
    "env_missing_settings = [\"MCAR\", \"Mcolor\", \"Mfog\"]\n",
    "env_missing = env_missing_settings[0]\n",
    "\n",
    "# for epsilon greedy, alpha, gamma in Q-learning\n",
    "epsilon = 0.05 # {0.0, 0.01, 0.05}\n",
    "alpha, gamma = 0.1, 0.1 # we'll tune alpha (0.01, 0.1, 0.25, 1.0) + gamma (0.1, 0.2)\n",
    "\n",
    "##### INITIALIZING START OF SIMULATIONS + DATA STRUCTURES\n",
    "\n",
    "# initialize our Q matrix: {((i, j, color), (a1, a2))}\n",
    "Q = gwh.init_Q(d)\n",
    "action_descs = gwh.load_actions()\n",
    "actions = list(action_descs.keys())\n",
    "\n",
    "# initialize our starting environment + corresponding colors\n",
    "env, env_colors = environments[ce][0], environments[ce][1]\n",
    "\n",
    "# initialize our true initial state to be the bottom left corner. Assume fully-observed initial state\n",
    "true_state = (d-1, 0, env_colors[d-1, 0])\n",
    "pobs_state, impu_state = true_state, true_state\n",
    "\n",
    "# initialize variable for our last fully-obs-state\n",
    "last_fobs_state = copy.deepcopy(true_state)\n",
    "\n",
    "'''\n",
    "DataFrame to log our results for this simulation:\n",
    "1. Mean reward per episode, # of times we landed in the river per episode, # of steps per episode.\n",
    "2. Counts of fully-observed, 1-missing, 2-missing, and 3-missing states per episode.\n",
    "3. Wall clock time per episode.\n",
    "'''\n",
    "# ALL METRICS ARE PER EPISODE!\n",
    "logs = pd.DataFrame(data=None, columns=[\"total_reward\", \"steps_river\", \"path_length\", \n",
    "                                        \"counts_0miss\", \"counts_1miss\", \"counts_2miss\", \"counts_3miss\",\n",
    "                                        \"wall_clock_time\"])\n",
    "\n",
    "# things we want to store PER EPISODE\n",
    "total_reward, steps_river, path_length = 0, 0, 0\n",
    "counts_0miss, counts_1miss, counts_2miss, counts_3miss = 0, 0, 0, 0\n",
    "wall_clock_time = None\n",
    "\n",
    "# start our timer FOR THIS EPISODE\n",
    "start_time = time.time()\n",
    "\n",
    "##### PROCEED FOR EACH TIMESTEP\n",
    "\n",
    "# for each timestep ...\n",
    "for t_step in range(max_iters):\n",
    "    \n",
    "    ##### \"choose action A from S using policy-derived from Q (e.g., \\epsilon-greedy)\"\n",
    "    \n",
    "    # do we have any missing state values?\n",
    "    if np.any(np.isnan(pobs_state).mean()):\n",
    "        \n",
    "        # deal with it accordingly to get imputed actions\n",
    "        if impute_method == \"last_fobs\":\n",
    "            action = gwh.select_action(last_fobs_state, Q, epsilon)\n",
    "        elif impute_method == \"random_action\":\n",
    "            action = actions[np.random.choice(a=len(actions))]\n",
    "        elif impute_method == \"missing_state\":\n",
    "            \n",
    "            # for this baseline method only, we need to convert np.nan to -1\n",
    "            pobs_state_temp = tuple([val if ~np.isnan(val) else -1 for val in pobs_state])\n",
    "            action = gwh.select_action(pobs_state_temp, Q, epsilon)\n",
    "        else:\n",
    "            raise Exception(\"impute_method choice is not currently supported.\")\n",
    "        \n",
    "    # if not, update our last_fobs_state + select an action accordingly\n",
    "    else:\n",
    "        \n",
    "        # select our action using standard epsilon-greedy on Q\n",
    "        action = gwh.select_action(pobs_state, Q, epsilon)\n",
    "    \n",
    "    ##### \"Take action A, observe R, S'\" - BASED ON TRUE STATE, OF COURSE!\n",
    "    \n",
    "    # toggle our environment potentially!\n",
    "    env, env_colors = environments[gwh.get_environment(ce, p_switch, indices)]\n",
    "    \n",
    "    # figure out what our new state is, which tells us our reward\n",
    "    new_true_state = gwh.true_move(true_state, action, env, env_colors, p_wind_i, p_wind_j)\n",
    "    reward = env[int(new_true_state[0]), int(new_true_state[1])]\n",
    "    \n",
    "    # update our reward counter + river counters\n",
    "    total_reward += reward\n",
    "    if reward == -10:\n",
    "        steps_river += 1\n",
    "    \n",
    "    # simulate our partially-observed mechanism.\n",
    "    if env_missing == \"MCAR\":\n",
    "        new_pobs_state = gwh.MCAR(new_true_state, thetas)\n",
    "    elif env_missing == \"Mcolor\":\n",
    "        new_pobs_state = gwh.Mcolor(new_true_state, theta_dict)\n",
    "    elif env_missing == \"Mfog\":\n",
    "        new_pobs_state = gwh.Mfog(new_true_state, i_range, j_range, thetas_in, thetas_out)\n",
    "    else:\n",
    "        raise Exception(\"The given env_missing mode is not supported.\")\n",
    "        \n",
    "    # make our imputation for the new_pobs_state, if not everything is observed.\n",
    "    if np.any(np.isnan(np.array(new_pobs_state)).mean()):\n",
    "    \n",
    "        if impute_method == \"last_fobs\":\n",
    "            new_impu_state = copy.deepcopy(last_fobs_state)\n",
    "        elif impute_method == \"random_action\":\n",
    "            new_impu_state = None # we're not imputing any states!\n",
    "        elif impute_method == \"missing_state\":\n",
    "            \n",
    "            # swapping np.nan to -1 to play nicer with dictionary indexing.\n",
    "            new_impu_state = tuple([val if ~np.isnan(val) else -1 for val in new_pobs_state])\n",
    "        else:\n",
    "            raise Exception(\"impute_method choice is not currently supported.\")\n",
    "    \n",
    "    # if nothing is missing, just set new_impu_state equal to the new_pobs_state\n",
    "    else:\n",
    "        \n",
    "        # just make a deepcopy!\n",
    "        new_impu_state = copy.deepcopy(new_pobs_state)\n",
    "    \n",
    "    # update our Q functions if permitted\n",
    "    \n",
    "    #TODO: will need to feed alpha = alpha/K for the multiple imputation stuff \n",
    "    \n",
    "    if impute_method != \"random_action\":\n",
    "        Q = gwh.update_Q(Q, impu_state, action, reward, new_impu_state, alpha, gamma)\n",
    "    elif ~np.any(np.isnan(new_pobs_state)):\n",
    "        if ~np.any(np.isnan(pobs_state)):\n",
    "            Q = gwh.update_Q(Q, pobs_state, action, reward, new_pobs_state, alpha, gamma)\n",
    "        \n",
    "    # check whether our last_fobs_state can be updated\n",
    "    if ~np.any(np.isnan(pobs_state).mean()):\n",
    "        last_fobs_state = copy.deepcopy(pobs_state)\n",
    "    \n",
    "    # update variable names for true_state, pobs_state, impu_state\n",
    "    true_state = copy.deepcopy(new_true_state)\n",
    "    pobs_state = copy.deepcopy(new_pobs_state)\n",
    "    impu_state = copy.deepcopy(new_impu_state)\n",
    "    \n",
    "    # update our missing data counters\n",
    "    globals()[f\"counts_{np.isnan(pobs_state).sum()}miss\"] += 1\n",
    "    \n",
    "    # update our path-length counter\n",
    "    path_length += 1\n",
    "    \n",
    "    # also see if we hit the terminal state\n",
    "    if (true_state[0] == 6) and (true_state[1] == 7):\n",
    "\n",
    "        # end our timer + record time elapsed FOR THIS EPISODE!\n",
    "        end_time = time.time()\n",
    "        wall_clock_time = end_time - start_time\n",
    "        \n",
    "        # update our dataframe\n",
    "        row = [total_reward, steps_river, path_length, \n",
    "               counts_0miss, counts_1miss, counts_2miss, counts_3miss,\n",
    "               wall_clock_time]\n",
    "        logs.loc[len(logs.index)] = row\n",
    "        \n",
    "        # reset our counter variables per EPISODE\n",
    "        total_reward, steps_river, path_length = 0, 0, 0\n",
    "        counts_0miss, counts_1miss, counts_2miss, counts_3miss = 0, 0, 0, 0\n",
    "        wall_clock_time = None\n",
    "        \n",
    "        # reset our timer, too\n",
    "        start_time = time.time()\n",
    "    \n",
    "    # status update?\n",
    "    if verbose == True:\n",
    "        if (t_step+1) % 5 == 0 and len(logs.index) >= 20:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Timestep: {t_step+1}, Past 20 Mean Epi. Sum Reward: {np.round(logs.loc[-20:].total_reward.mean(), 3)}, Fin. Episodes: {len(logs.index)}, Past 20 Mean Path Length: {np.round(logs.loc[-20:].path_length.mean(), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3861599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have a results folder\n",
    "if \"results\" not in os.listdir():\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "# start our filename: p_switch = PS, PW = p_wind_{i,j}, MM = missingness mechanism\n",
    "fname = f\"PS={p_switch}_PW={p_wind_i}_MM={env_missing}\"\n",
    "\n",
    "# record the MCAR variables\n",
    "if env_missing == \"MCAR\":\n",
    "\n",
    "    # all theta_i are the same, just record what the theta was.\n",
    "    fname += f\"_theta={thetas[0]}\"\n",
    "\n",
    "# record the Mcolor variables\n",
    "elif env_missing == \"Mcolor\":\n",
    "    \n",
    "    # only thing that is differential/changing is whether the last value is 0.0 or something else.\n",
    "    fname += f\"_theta-color={theta_dict[0][2]}\"\n",
    "\n",
    "# record the Mfog variables    \n",
    "elif env_missing == \"Mfog\":\n",
    "    \n",
    "    # record fog-in and fog-out theta values (equal for each component)\n",
    "    fname += f\"_theta-in={thetas_in[0]}_theta-out={thetas_out[0]}\"\n",
    "    \n",
    "# else, throw a hissy fit\n",
    "else:\n",
    "    raise Exception(\"Missingness mechanism is not supported.\")\n",
    "    \n",
    "# add in our imputation mechanism\n",
    "IM_desc = impute_method.replace(\"_\", \"-\")\n",
    "fname += f\"_IM={IM_desc}\"\n",
    "\n",
    "# if applicable, Kyla's \"joint\" and \"mice\":\n",
    "if impute_method in [\"joint\", \"mice\"]:\n",
    "    \n",
    "    # encode NC: num_cycles, K: no. of imputations, PS: p_shuffle\n",
    "    fname += f\"_NC={num_cycles}_K={K}_p-shuffle={p_shuffle}\"\n",
    "    \n",
    "# add in number of maximum iterations\n",
    "fname += f\"_max-iters={max_iters}\"\n",
    "\n",
    "# add in final hyperparameters: epsilon, alpha, gamma\n",
    "fname += f\"_eps={epsilon}_alpha={alpha}_gamma={gamma}\"\n",
    "\n",
    "# make a directory for this foldername\n",
    "if fname not in os.listdir(\"results\"):\n",
    "    os.mkdir(f\"results/{fname}\")\n",
    "    \n",
    "# save the log file to a .csv\n",
    "logs.to_csv(f\"results/{fname}/seed={seed}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Afterburner)\n",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
