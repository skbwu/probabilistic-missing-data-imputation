{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4caf24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, copy, os, shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import copy, time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# our helper functions for the gridworlds\n",
    "import GridWorldHelpers as gwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc87fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our environments + corresponding colors\n",
    "d = 8 # dimension of our \n",
    "gw0, gw1, gw2 = gwh.build_grids(d=8, baseline_penalty = -1, \n",
    "                                water_penalty = -10, \n",
    "                                end_reward = 100)\n",
    "gw0_colors = gwh.make_gw_colors(gw0)\n",
    "gw1_colors = gwh.make_gw_colors(gw1)\n",
    "gw2_colors = gwh.make_gw_colors(gw2)\n",
    "\n",
    "# store quick-access indices for the environment\n",
    "environments = {\n",
    "                0: [gw0, gw0_colors], # baseline\n",
    "                1: [gw1, gw2_colors], # non-flooding\n",
    "                2: [gw2, gw2_colors] # flooding\n",
    "               }\n",
    "\n",
    "# global environment parameters \n",
    "p_switch = 0.1 # flooding Markov chain parameter, {0.0, 0.1}\n",
    "p_wind_i = 0.0 # up-down wind frequency, {0, 0.1, 0.2} (COUPLED WITH p_wind_j)\n",
    "p_wind_j = 0.0 # left-right wind frequency, {0, 0.1, 0.2}\n",
    "\n",
    "# global missing-data parameters\n",
    "thetas = np.array([0.0, 0.0, 0.0]) # common thetas -- {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}\n",
    "thetas_in = np.array([0.50, 0.50, 0.50]) # (0.5, 0.5, 0.5) + (0.3, 0.3, 0.3)\n",
    "thetas_out = np.array([0.1, 0.1, 0.1]) # (0.0, 0.0, 0.0) + (0.1, 0.1, 0.1)\n",
    "\n",
    "# for each color\n",
    "theta_dict = {0 : np.array([0.1, 0.1, 0.1]), # 0.1 + option of (0.1, 0.1, 0.0)\n",
    "              1 : np.array([0.2, 0.2, 0.2]), # 0.2 + option of (0.2, 0.2, 0.0)\n",
    "              2 : np.array([0.3, 0.3, 0.3])} # 0.3 + option of (0.3, 0.3, 0.0)\n",
    "\n",
    "# fog range - fixed.\n",
    "i_range, j_range = (0, 2), (5, 7)\n",
    "\n",
    "# what is our starting \"current environment\"\n",
    "ce = 1\n",
    "\n",
    "# which environments are we flipping through?\n",
    "indices = np.array([1, 2]) # the two to be flipping between, if any. If just one, make first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e732adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 3275, Cum. Mean Reward: -0.545, Finished Episodes: 25, Past 20 Mean Path Length: 143.35\n"
     ]
    }
   ],
   "source": [
    "# set a seed\n",
    "np.random.seed(858)\n",
    "\n",
    "# make sure we are refreshing our helper functions\n",
    "reload(gwh)\n",
    "\n",
    "##### SIMULATION SETTINGS\n",
    "max_iters = 20000\n",
    "\n",
    "# which baseline imputing setting are we using? (note in random_action, do NOT update Q-matrix!)\n",
    "impute_method_settings = [\"last_fobs\", \"random_action\", \"missing_state\"]\n",
    "impute_method = impute_method_settings[1]\n",
    "\n",
    "# which missing-data setup are we using for environment?\n",
    "env_missing_settings = [\"MCAR\", \"Mcolor\", \"Mfog\"]\n",
    "env_missing = env_missing_settings[0]\n",
    "\n",
    "# for epsilon greedy, alpha, gamma in Q-learning\n",
    "epsilon = 0.05 # {0.0, 0.01, 0.05}\n",
    "alpha, gamma = 0.1, 0.1 # we'll tune alpha (0.01, 0.1, 0.25, 1.0) + gamma (0.1, 0.2)\n",
    "\n",
    "##### INITIALIZING START OF SIMULATIONS + DATA STRUCTURES\n",
    "\n",
    "# initialize our Q matrix: {((i, j, color), (a1, a2))}\n",
    "Q = gwh.init_Q(d)\n",
    "action_descs = gwh.load_actions()\n",
    "actions = list(action_descs.keys())\n",
    "\n",
    "# counter of how many episodes we've hit\n",
    "finished_episodes = 0\n",
    "\n",
    "# initialize our starting environment + corresponding colors\n",
    "env, env_colors = environments[ce][0], environments[ce][1]\n",
    "\n",
    "# initialize our true initial state to be the bottom left corner. Assume fully-observed initial state\n",
    "true_state = (d-1, 0, env_colors[d-1, 0])\n",
    "pobs_state, impu_state = true_state, true_state\n",
    "\n",
    "# things we want to store: rewards, time per iteration, last fully-observed state, Qmatrices every X steps.\n",
    "rewards, times, path_lengths, last_fobs_state = [], [], [], copy.deepcopy(true_state)\n",
    "\n",
    "##### PROCEED FOR EACH TIMESTEP\n",
    "\n",
    "# initialize our path_length\n",
    "path_length = 0\n",
    "\n",
    "# for each timestep ...\n",
    "for t_step in range(max_iters):\n",
    "    \n",
    "    # start our timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ##### \"choose action A from S using policy-derived from Q (e.g., \\epsilon-greedy)\"\n",
    "    \n",
    "    # do we have any missing state values?\n",
    "    if np.any(np.isnan(pobs_state).mean()):\n",
    "        \n",
    "        # deal with it accordingly to get imputed actions\n",
    "        if impute_method == \"last_fobs\":\n",
    "            action = gwh.select_action(last_fobs_state, Q, epsilon)\n",
    "        elif impute_method == \"random_action\":\n",
    "            action = actions[np.random.choice(a=len(actions))]\n",
    "        elif impute_method == \"missing_state\":\n",
    "            action = gwh.select_action(\"missing\", Q, epsilon)\n",
    "        else:\n",
    "            raise Exception(\"impute_method choice is not currently supported.\")\n",
    "        \n",
    "    # if not, update our last_fobs_state + select an action accordingly\n",
    "    else:\n",
    "        \n",
    "        # select our action using standard epsilon-greedy on Q\n",
    "        action = gwh.select_action(pobs_state, Q, epsilon)\n",
    "    \n",
    "    ##### \"Take action A, observe R, S'\" - BASED ON TRUE STATE, OF COURSE!\n",
    "    \n",
    "    # toggle our environment potentially!\n",
    "    env, env_colors = environments[gwh.get_environment(ce, p_switch, indices)]\n",
    "    \n",
    "    # figure out what our new state is, which tells us our reward\n",
    "    new_true_state = gwh.true_move(true_state, action, env, env_colors, p_wind_i, p_wind_j)\n",
    "    reward = env[int(new_true_state[0]), int(new_true_state[1])]\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # simulate our partially-observed mechanism.\n",
    "    if env_missing == \"MCAR\":\n",
    "        new_pobs_state = gwh.MCAR(new_true_state, thetas)\n",
    "    elif env_missing == \"Mcolor\":\n",
    "        new_pobs_state = gwh.Mcolor(new_true_state, theta_dict)\n",
    "    elif env_missing == \"Mfog\":\n",
    "        new_pobs_state = gwh.Mfog(new_true_state, i_range, j_range, thetas_in, thetas_out)\n",
    "    else:\n",
    "        raise Exception(\"The given env_missing mode is not supported.\")\n",
    "        \n",
    "    # make our imputation for the new_pobs_state, if not everything is observed.\n",
    "    if np.any(np.isnan(np.array(new_pobs_state)).mean()):\n",
    "    \n",
    "        if impute_method == \"last_fobs\":\n",
    "            new_impu_state = copy.deepcopy(last_fobs_state)\n",
    "        elif impute_method == \"random_action\":\n",
    "            new_impu_state = None # we're not imputing any states!\n",
    "        elif impute_method == \"missing_state\":\n",
    "            new_impu_state = \"missing\"\n",
    "        else:\n",
    "            raise Exception(\"impute_method choice is not currently supported.\")\n",
    "    \n",
    "    # if nothing is missing, just set new_impu_state equal to the new_pobs_state\n",
    "    else:\n",
    "        \n",
    "        # just make a deepcopy!\n",
    "        new_impu_state = copy.deepcopy(new_pobs_state)\n",
    "    \n",
    "    # update our Q functions if permitted\n",
    "    \n",
    "    #TODO: will need to feed alpha = alpha/K for the multiple imputation stuff \n",
    "    \n",
    "    if impute_method != \"random_action\":\n",
    "        Q = gwh.update_Q(Q, impu_state, action, reward, new_impu_state, alpha, gamma)\n",
    "    elif ~np.any(np.isnan(new_pobs_state)):\n",
    "        if ~np.any(np.isnan(pobs_state)):\n",
    "            Q = gwh.update_Q(Q, pobs_state, action, reward, new_pobs_state, alpha, gamma)\n",
    "        \n",
    "    # check whether our last_fobs_state can be updated\n",
    "    if ~np.any(np.isnan(pobs_state).mean()):\n",
    "        last_fobs_state = copy.deepcopy(pobs_state)\n",
    "    \n",
    "    # update variable names for true_state, pobs_state, impu_state\n",
    "    true_state = copy.deepcopy(new_true_state)\n",
    "    pobs_state = copy.deepcopy(new_pobs_state)\n",
    "    impu_state = copy.deepcopy(new_impu_state)\n",
    "    \n",
    "    # end our timer + record time elapsed\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "    \n",
    "    # update our counter\n",
    "    path_length += 1\n",
    "    \n",
    "    # also see if we hit the terminal state\n",
    "    if (true_state[0] == 6) and (true_state[1] == 7):\n",
    "        finished_episodes += 1\n",
    "        \n",
    "        # record our path_length\n",
    "        path_lengths.append(path_length)\n",
    "        path_length = 0\n",
    "    \n",
    "    # status update?\n",
    "    if (t_step+1) % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Timestep: {t_step+1}, Cum. Mean Reward: {np.round(np.mean(rewards), 3)}, Finished Episodes: {finished_episodes}, Past 20 Mean Path Length: {np.round(np.mean(path_lengths[-20:]), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean reward per episode, no. of times in river per episode, no. of steps per episodes.\n",
    "# no. of fully-observed states per episode, no. of 1-missing states, no. of 2-missing states, no. of all-missing states\n",
    "# wall-clock time per episode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Afterburner)\n",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
