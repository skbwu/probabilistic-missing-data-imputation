{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4caf24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, copy, os, shutil\n",
    "import copy, time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# our helper functions for the gridworlds\n",
    "import GridWorldHelpers as gwh\n",
    "import GridWorldImputers as gwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be19dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample of what theta_dict should look like:\n",
    "# for each color\n",
    "theta_dict = {0 : np.array([0.1, 0.1, 0.1]), # 0.1 + option of (0.1, 0.1, 0.0)\n",
    "              1 : np.array([0.2, 0.2, 0.2]), # 0.2 + option of (0.2, 0.2, 0.0)\n",
    "              2 : np.array([0.3, 0.3, 0.3])} # 0.3 + option of (0.3, 0.3, 0.0)\n",
    "'''\n",
    "\n",
    "# create a master function\n",
    "def runner(p_switch, # float, flooding Markov chain parameter, {0.0, 0.1}\n",
    "           p_wind_i, p_wind_j, # float, up-down/left-right wind frequency, {0.0, 0.1, 0.2}. INTENDED EQUAL!\n",
    "           env_missing, # environment-missingness governor \"MCAR\", \"Mcolor\", \"Mfog\"\n",
    "           thetas, # np.array, MCAR, same theta_i values {0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}\n",
    "           thetas_in, # np.array, Mfog, in: (0.5, 0.5, 0.5) + (0.25, 0.25, 0.25)\n",
    "           thetas_out, # np.array, Mfog, out: (0.0, 0.0, 0.0) + (0.1, 0.1, 0.1)\n",
    "           theta_dict, # dict with keys {0, 1, 2} corresponding to a np.array each.\n",
    "           impute_method, # \"last_fobs\", \"random_action\", \"missing_state\", \"joint\", \"mice\"\n",
    "           K, #number of multiple imputation chains\n",
    "           p_shuffle, #shuffle rate for chains\n",
    "           num_cycles, #number of cycles used in Mice\n",
    "           epsilon, # epsilon-greedy governor {0.0, 0.01, 0.05}\n",
    "           alpha, # learning rate (0.1, 1.0)\n",
    "           gamma, # discount factor (0.0, 0.25, 0.5, 0.75, 1.0)\n",
    "           max_iters, # how many iterations are we going for?\n",
    "           seed, # randomization seed\n",
    "           verbose=False): # intermediate outputs or nah?\n",
    "\n",
    "    # For convenience\n",
    "    MImethods = [\"joint\",\"mice\"]\n",
    "    \n",
    "    ###############################################################\n",
    "    ##### CREATING ENVIRONMENT + SETTING THE SEED #################\n",
    "    ###############################################################\n",
    "    assert K >= 1 or K is None\n",
    "    assert num_cycles >= 1 or num_cycles is None\n",
    "    \n",
    "    # initializing our environments + corresponding colors\n",
    "    d = 8 # dimension of our gridworld\n",
    "    colors = [0,1,2] #colors encoded with 0,1,2\n",
    "    gw0, gw1, gw2 = gwh.build_grids(d=8, baseline_penalty = -1, \n",
    "                                    water_penalty = -10, \n",
    "                                    end_reward = 100)\n",
    "    gw0_colors = gwh.make_gw_colors(gw0)\n",
    "    gw1_colors = gwh.make_gw_colors(gw1)\n",
    "    gw2_colors = gwh.make_gw_colors(gw2)\n",
    "\n",
    "    # store quick-access indices for the environment\n",
    "    environments = {\n",
    "                    0: [gw0, gw0_colors], # baseline\n",
    "                    1: [gw1, gw2_colors], # non-flooding\n",
    "                    2: [gw2, gw2_colors] # flooding\n",
    "                   }\n",
    "    \n",
    "    # fog range - fixed.\n",
    "    i_range, j_range = (0, 2), (5, 7)\n",
    "\n",
    "    # what is our starting \"current environment\"\n",
    "    ce = 1\n",
    "\n",
    "    # which environments are we flipping through?\n",
    "    indices = np.array([1, 2]) # the two to be flipping between, if any. If just one, make first element\n",
    "    \n",
    "    # set our seed for use in multiple trials\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    ###############################################################\n",
    "    ##### INITIALIZING START OF SIMULATIONS + DATA STRUCTURES #####\n",
    "    ###############################################################\n",
    "    \n",
    "    # initialize our Q matrix: {((i, j, color), (a1, a2))}\n",
    "    if impute_method == \"missing_state\":\n",
    "        Q = gwh.init_Q(d, include_missing_as_state=True, colors = colors)\n",
    "    else:\n",
    "        Q = gwh.init_Q(d, include_missing_as_state=False, colors = colors)\n",
    "\n",
    "    # initialize Transition matrices\n",
    "    Tstandard = gwi.init_Tstandard(d, colors, 0)\n",
    "    Tmice = gwi.init_Tmice(d, colors, 0)\n",
    "\n",
    "    # load the possible actions list\n",
    "    action_descs = gwh.load_actions()\n",
    "    actions = list(action_descs.keys())\n",
    "\n",
    "    # initialize our starting environment + corresponding colors\n",
    "    env, env_colors = environments[ce][0], environments[ce][1]\n",
    "\n",
    "    # initialize our true initial state to be the bottom left corner.\n",
    "    true_state = (d-1, 0, env_colors[d-1, 0])\n",
    "    #  Assume fully-observed initial state and initialize first obs\n",
    "    #  state and first imp state\n",
    "    pobs_state, impu_state = true_state, true_state\n",
    "\n",
    "    # if doing multiple imputation method, initilize state list\n",
    "    imp_state_list = [true_state] * K\n",
    "    \n",
    "\n",
    "    # initialize variable for our last fully-obs-state\n",
    "    last_fobs_state = copy.deepcopy(true_state)\n",
    "\n",
    "    '''\n",
    "    DataFrame to log our results for this simulation:\n",
    "    1. Mean reward per episode, # of times we landed in the river per episode, # of steps per episode.\n",
    "    2. Counts of fully-observed, 1-missing, 2-missing, and 3-missing states per episode.\n",
    "    3. Wall clock time per episode.\n",
    "    '''\n",
    "    # ALL METRICS ARE PER EPISODE!\n",
    "    logs = pd.DataFrame(data=None, columns=[\"total_reward\", \"steps_river\", \"path_length\", \n",
    "                                            \"counts_0miss\", \"counts_1miss\", \"counts_2miss\", \"counts_3miss\",\n",
    "                                            \"wall_clock_time\"])\n",
    "\n",
    "    # things we want to store PER EPISODE\n",
    "    total_reward, steps_river, path_length = 0, 0, 0\n",
    "    counts_0miss, counts_1miss, counts_2miss, counts_3miss = 0, 0, 0, 0\n",
    "    wall_clock_time = None\n",
    "\n",
    "    # start our timer FOR THIS EPISODE\n",
    "    start_time = time.time()\n",
    "\n",
    "    ###############################################################\n",
    "    ##### RUNNING SIMULATIONS FOR EACH TIMESTEP ###################\n",
    "    ###############################################################\n",
    "\n",
    "    # for each timestep ...\n",
    "    for t_step in range(max_iters):\n",
    "\n",
    "\n",
    "        #############################################################\n",
    "        # Action selection based on last state(s) or random selection \n",
    "        #############################################################\n",
    "        # \"choose action A from S using policy-derived from Q (e.g., \\epsilon-greedy)\"\n",
    "\n",
    "        # do we have any missing state values?\n",
    "        if np.any(np.isnan(pobs_state).mean()):\n",
    "            # deal with it accordingly to get imputed actions\n",
    "            if impute_method == \"last_fobs\":\n",
    "                action = gwh.select_action(last_fobs_state, Q, epsilon)\n",
    "            elif impute_method == \"random_action\":\n",
    "                action = actions[np.random.choice(a=len(actions))]\n",
    "            elif impute_method == \"missing_state\":\n",
    "                # for this method only, we need to convert np.nan to -1\n",
    "                pobs_state_temp = tuple([val if ~np.isnan(val) else -1 for val in pobs_state])\n",
    "                action = gwh.select_action(pobs_state_temp, Q, epsilon)\n",
    "            elif impute_method in MImethods:\n",
    "                # vote on action. note: not taking most-selected action because suspect not enough exploration\n",
    "                actions = [gwh.select_action(s, Q, epsilon) for s in imp_state_list]\n",
    "                action = np.random.choice(actions)                   \n",
    "            else:\n",
    "                raise Exception(\"impute_method choice is not currently supported.\")\n",
    "        # if no missingness, select an action by standard epsilon greedy \n",
    "        else:\n",
    "            action = gwh.select_action(pobs_state, Q, epsilon)\n",
    "\n",
    "\n",
    "        ###############################################\n",
    "        # Take action A, observe R, S'\n",
    "        # Taking action affects underlying TRUE state, even if\n",
    "        # we won't observe it!!!\n",
    "        ###############################################\n",
    "\n",
    "        # toggle our environment potentially!\n",
    "        env, env_colors = environments[gwh.get_environment(ce, p_switch, indices)]\n",
    "\n",
    "        # figure out what our new state is, which tells us our reward\n",
    "        new_true_state = gwh.true_move(true_state, action, env, env_colors, p_wind_i, p_wind_j)\n",
    "        reward = env[int(new_true_state[0]), int(new_true_state[1])]\n",
    "\n",
    "        # update our reward counter + river counters\n",
    "        total_reward += reward\n",
    "        if reward == -10:\n",
    "            steps_river += 1\n",
    "\n",
    "        ###############################################\n",
    "        # Apply missingness mechanism to generate our new partially observed state\n",
    "        ###############################################\n",
    "\n",
    "        # simulate our partially-observed mechanism.\n",
    "        if env_missing == \"MCAR\":\n",
    "            new_pobs_state = gwh.MCAR(new_true_state, thetas)\n",
    "        elif env_missing == \"Mcolor\":\n",
    "            new_pobs_state = gwh.Mcolor(new_true_state, theta_dict)\n",
    "        elif env_missing == \"Mfog\":\n",
    "            new_pobs_state = gwh.Mfog(new_true_state, i_range, j_range, thetas_in, thetas_out)\n",
    "        else:\n",
    "            raise Exception(\"The given env_missing mode is not supported.\")\n",
    "\n",
    "        ###############################################\n",
    "        # IMPUTATION\n",
    "        # make our imputation for the new_pobs_state, if not everything is observed.\n",
    "        ###############################################\n",
    "\n",
    "        if np.any(np.isnan(np.array(new_pobs_state)).mean()):\n",
    "\n",
    "            if impute_method == \"last_fobs\":\n",
    "                new_impu_state = copy.deepcopy(last_fobs_state)\n",
    "            \n",
    "            elif impute_method == \"random_action\":\n",
    "                new_impu_state = None # we're not imputing any states!\n",
    "            \n",
    "            elif impute_method == \"missing_state\":\n",
    "\n",
    "                # swapping np.nan to -1 to play nicer with dictionary indexing.\n",
    "                new_impu_state = tuple([val if ~np.isnan(val) else -1 for val in new_pobs_state])\n",
    "\n",
    "            elif impute_method in MImethods:\n",
    "\n",
    "                #decide if we will shuffle (affects Q and T updates below)\n",
    "                shuffle = gwi.shuffle(p_shuffle)\n",
    "\n",
    "                #generate list of imputed values\n",
    "                #note: because first state already observed, will only\n",
    "                #get here when already have defined action variable \n",
    "                new_imp_state_list = gwi.MI(\n",
    "                       method = impute_method,\n",
    "                       Slist = imp_state_list,\n",
    "                       A = action, #previous action?\n",
    "                       pobs_state = new_pobs_state,\n",
    "                       shuffle = shuffle,\n",
    "                       Tmice = Tmice,\n",
    "                       Tstandard = Tstandard,\n",
    "                       num_cycles = num_cycles)\n",
    "            else:\n",
    "                raise Exception(\"impute_method choice is not currently supported.\")\n",
    "\n",
    "        # if nothing is missing, just set new_impu_state equal to the new_pobs_state\n",
    "        else:\n",
    "            # just make a deepcopy!\n",
    "            new_impu_state = copy.deepcopy(new_pobs_state)\n",
    "\n",
    "        ######################################\n",
    "        # Q update (if permitted)\n",
    "        ######################################\n",
    "        # multiple imputation way of updating Q with fractional allocation\n",
    "        if impute_method in MImethods:\n",
    "            Q  = gwi.updateQ_MI(Q, \n",
    "                                Slist = imp_state_list, \n",
    "                                new_Slist = new_imp_state_list, \n",
    "                                A = action, reward = reward, \n",
    "                                alpha = alpha, gamma = gamma)\n",
    "            \n",
    "        # if we have random_action method, then we cannot update \n",
    "        elif impute_method != \"random_action\":\n",
    "            Q = gwh.update_Q(Q, impu_state, action, reward, new_impu_state, alpha, gamma)\n",
    "    \n",
    "        #if nothing is missing in last or current state, then we can\n",
    "        #update Q under random_action\n",
    "        elif ~np.any(np.isnan(new_pobs_state)):\n",
    "            if ~np.any(np.isnan(pobs_state)):\n",
    "                Q = gwh.update_Q(Q, pobs_state, action, reward, new_pobs_state, alpha, gamma)\n",
    "\n",
    "        ######################################\n",
    "        # T update (if needed)\n",
    "        ######################################\n",
    "        if impute_method in MImethods:\n",
    "            if impute_method == \"mice\":\n",
    "                gwi.Tmice_update(Tmice, \n",
    "                                 Slist = imp_state_list, \n",
    "                                 A = action, \n",
    "                                 newSlist = new_imp_state_list)\n",
    "            if impute_method == \"joint\":\n",
    "                gwi.Tstandard_update(Tstandard, \n",
    "                                     Slist = imp_state_list,\n",
    "                                     A = action,\n",
    "                                     new_Slist = new_imp_state_list)\n",
    "\n",
    "        # check whether our last_fobs_state can be updated\n",
    "        if ~np.any(np.isnan(pobs_state).mean()):\n",
    "            last_fobs_state = copy.deepcopy(pobs_state)\n",
    "\n",
    "        # now that we have updated Q and T functions\n",
    "        # update true_state, pobs_state, impu_state, imp_state_list\n",
    "        # as 'current state' for for the next round\n",
    "        true_state = copy.deepcopy(new_true_state)\n",
    "        pobs_state = copy.deepcopy(new_pobs_state)\n",
    "        impu_state = copy.deepcopy(new_impu_state)\n",
    "        if impute_method in MImethods:\n",
    "            imp_state_list = copy.deepcopy(new_imp_state_list)\n",
    "        \n",
    "        # update our missing data counters\n",
    "        if np.isnan(pobs_state).sum() == 0:\n",
    "            counts_0miss += 1\n",
    "        elif np.isnan(pobs_state).sum() == 1:\n",
    "            counts_1miss += 1\n",
    "        elif np.isnan(pobs_state).sum() == 2:\n",
    "            counts_2miss += 1\n",
    "        elif np.isnan(pobs_state).sum() == 3:\n",
    "            counts_3miss += 1\n",
    "\n",
    "        # update our path-length counter\n",
    "        path_length += 1\n",
    "\n",
    "        # also see if we hit the terminal state\n",
    "        if (true_state[0] == 6) and (true_state[1] == 7):\n",
    "\n",
    "            # end our timer + record time elapsed FOR THIS EPISODE!\n",
    "            end_time = time.time()\n",
    "            wall_clock_time = end_time - start_time\n",
    "\n",
    "            # update our dataframe\n",
    "            row = [total_reward, steps_river, path_length, \n",
    "                   counts_0miss, counts_1miss, counts_2miss, counts_3miss,\n",
    "                   wall_clock_time]\n",
    "            logs.loc[len(logs.index)] = row\n",
    "\n",
    "            # reset our counter variables per EPISODE\n",
    "            total_reward, steps_river, path_length = 0, 0, 0\n",
    "            counts_0miss, counts_1miss, counts_2miss, counts_3miss = 0, 0, 0, 0\n",
    "            wall_clock_time = None\n",
    "\n",
    "            # reset our timer, too\n",
    "            start_time = time.time()\n",
    "\n",
    "        # status update?\n",
    "        if verbose == True:\n",
    "            if (t_step+1) % 5 == 0 and len(logs.index) >= 20:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Timestep: {t_step+1}, Past 20 Mean Epi. Sum Reward: {np.round(logs.loc[-20:].total_reward.mean(), 3)}, Fin. Episodes: {len(logs.index)}, Past 20 Mean Path Length: {np.round(logs.loc[-20:].path_length.mean(), 3)}\")\n",
    "                \n",
    "    ###############################################################\n",
    "    ##### SAVING OUR LOGS FILE TO A .CSV OUTPUT ###################\n",
    "    ###############################################################\n",
    "    \n",
    "    # check if we have a results folder\n",
    "    if \"results\" not in os.listdir():\n",
    "        os.mkdir(\"results\")\n",
    "\n",
    "    # start our filename: p_switch = PS, PW = p_wind_{i,j}, MM = missingness mechanism\n",
    "    fname = f\"PS={p_switch}_PW={p_wind_i}_MM={env_missing}\"\n",
    "\n",
    "    # record the MCAR variables\n",
    "    if env_missing == \"MCAR\":\n",
    "\n",
    "        # all theta_i are the same, just record what the theta was.\n",
    "        fname += f\"_theta={thetas[0]}\"\n",
    "\n",
    "    # record the Mcolor variables\n",
    "    elif env_missing == \"Mcolor\":\n",
    "\n",
    "        # only thing that is differential/changing is whether the last value is 0.0 or something else.\n",
    "        fname += f\"_t-color={theta_dict[0][2]}\"\n",
    "\n",
    "    # record the Mfog variables    \n",
    "    elif env_missing == \"Mfog\":\n",
    "\n",
    "        # record fog-in and fog-out theta values (equal for each component)\n",
    "        fname += f\"_t-in={thetas_in[0]}_t-out={thetas_out[0]}\"\n",
    "\n",
    "    # else, throw a hissy fit\n",
    "    else:\n",
    "        raise Exception(\"Missingness mechanism is not supported.\")\n",
    "\n",
    "    # add in our imputation mechanism\n",
    "    IM_desc = impute_method.replace(\"_\", \"-\")\n",
    "    fname += f\"_IM={IM_desc}\"\n",
    "\n",
    "    if impute_method in MImethods:\n",
    "        # encode NC: num_cycles, K: no. of imputations, PS: p_shuffle\n",
    "        fname += f\"_NC={num_cycles}_K={K}_p-shuf={p_shuffle}\"\n",
    "\n",
    "    # add in number of maximum iterations\n",
    "    fname += f\"_max-iters={max_iters}\"\n",
    "\n",
    "    # add in final hyperparameters: epsilon, alpha, gamma\n",
    "    fname += f\"_eps={epsilon}_a={alpha}_g={gamma}\"\n",
    "\n",
    "    # make a directory for this foldername\n",
    "    if fname not in os.listdir(\"results\"):\n",
    "        os.mkdir(f\"results/{fname}\")\n",
    "\n",
    "    # save the log file to a .csv\n",
    "    logs.to_csv(f\"results/{fname}/seed={seed}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19902fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 2000, Past 20 Mean Epi. Sum Reward: 13.818, Fin. Episodes: 33, Past 20 Mean Path Length: 60.455\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 206] The filename or extension is too long: 'results/PS=0.1_PW=0.1_MM=Mfog_t-in=0.5_t-out=0.0_IM=missing-state_max-iters=2000_eps=0.05_a=1.0_g=0.5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_switch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# float, flooding Markov chain parameter, {0.0, 0.1}\u001b[39;49;00m\n\u001b[0;32m      2\u001b[0m \u001b[43m       \u001b[49m\u001b[43mp_wind_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_wind_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# float, up-down/left-right wind frequency, {0.0, 0.1, 0.2}. INTENDED EQUAL!\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m       \u001b[49m\u001b[43menv_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMfog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# environment-missingness governor \"MCAR\", \"Mcolor\", \"Mfog\"\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m       \u001b[49m\u001b[43mthetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# np.array, MCAR, same theta_i values {0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m       \u001b[49m\u001b[43mthetas_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# np.array, Mfog, in: (0.5, 0.5, 0.5) + (0.25, 0.25, 0.25)\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m       \u001b[49m\u001b[43mthetas_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# np.array, Mfog, out: (0.0, 0.0, 0.0) + (0.1, 0.1, 0.1)\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m       \u001b[49m\u001b[43mtheta_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dict with keys {0, 1, 2} corresponding to a np.array each.\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m       \u001b[49m\u001b[43mimpute_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmissing_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# \"last_fobs\", \"random_action\", \"missing_state\", \"joint\", \"mice\"\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m       \u001b[49m\u001b[43mp_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m       \u001b[49m\u001b[43mnum_cycles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m       \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# for mice + joint: shuffle chains, no. of cycles, no. impute\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m       \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# epsilon-greedy governor {0.0, 0.01, 0.05}\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m       \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# learning rate (0.1, 1.0)\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m       \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# discount factor (0.0, 0.25, 0.5, 0.75, 1.0)\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m       \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# how many iterations are we going for?\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m       \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# randomization seed\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m       \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# intermediate outputs or nah?\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 367\u001b[0m, in \u001b[0;36mrunner\u001b[1;34m(p_switch, p_wind_i, p_wind_j, env_missing, thetas, thetas_in, thetas_out, theta_dict, impute_method, K, p_shuffle, num_cycles, epsilon, alpha, gamma, max_iters, seed, verbose)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# make a directory for this foldername\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 367\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# save the log file to a .csv\u001b[39;00m\n\u001b[0;32m    370\u001b[0m logs\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/seed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 206] The filename or extension is too long: 'results/PS=0.1_PW=0.1_MM=Mfog_t-in=0.5_t-out=0.0_IM=missing-state_max-iters=2000_eps=0.05_a=1.0_g=0.5'"
     ]
    }
   ],
   "source": [
    "runner(p_switch=0.1, # float, flooding Markov chain parameter, {0.0, 0.1}\n",
    "       p_wind_i=0.1, p_wind_j=0.1, # float, up-down/left-right wind frequency, {0.0, 0.1, 0.2}. INTENDED EQUAL!\n",
    "       env_missing=\"Mfog\", # environment-missingness governor \"MCAR\", \"Mcolor\", \"Mfog\"\n",
    "       thetas=None, # np.array, MCAR, same theta_i values {0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}\n",
    "       thetas_in=np.array([0.5, 0.5, 0.5]), # np.array, Mfog, in: (0.5, 0.5, 0.5) + (0.25, 0.25, 0.25)\n",
    "       thetas_out=np.array([0.0, 0.0, 0.0]), # np.array, Mfog, out: (0.0, 0.0, 0.0) + (0.1, 0.1, 0.1)\n",
    "       theta_dict=None, # dict with keys {0, 1, 2} corresponding to a np.array each.\n",
    "       impute_method=\"missing_state\", # \"last_fobs\", \"random_action\", \"missing_state\", \"joint\", \"mice\"\n",
    "       p_shuffle=0, \n",
    "       num_cycles=10, \n",
    "       K=10, # for mice + joint: shuffle chains, no. of cycles, no. impute\n",
    "       epsilon=0.05, # epsilon-greedy governor {0.0, 0.01, 0.05}\n",
    "       alpha=1.0, # learning rate (0.1, 1.0)\n",
    "       gamma=0.5, # discount factor (0.0, 0.25, 0.5, 0.75, 1.0)\n",
    "       max_iters=2000, # how many iterations are we going for?\n",
    "       seed=0, # randomization seed\n",
    "       verbose=True) # intermediate outputs or nah?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ef23b-79b9-4629-a96b-0889e4b74f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
